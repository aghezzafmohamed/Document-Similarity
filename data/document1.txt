The beld of study that focuses on the interactions between human language
and computers is called Natural Language Processing, or NLP for short. It sits
at the intersection of computer science, artibcial intelligence, and
computational linguistics[Wikipedia].NLP is a way for computers to analyze,
understand, and derive meaning from human language in a smart and useful
way. By utilizing NLP, developers can organize and structure knowledge to
perform tasks such as automatic summarization, translation, named entity
recognition, relationship extraction, sentiment analysis, speech recognition,
and topic segmentation.
NLTK: A Brief Intro
NLTK(Natural Language Toolkit) is a leading platform for building Python
programs to work with human language data. It provides easy-to-use
interfaces to over 50 corpora and lexical resources such as WordNet, along
with a suite of text processing libraries for classibcation, tokenization,
stemming, tagging, parsing, and semantic reasoning, wrappers for industrialstrength NLP libraries.
NLTK has been called “a wonderful tool for teaching and working in,
computational linguistics using Python,” and “an amazing library to play with
natural language.”
Natural Language Processing with Python provides a practical introduction
to programming for language processing. I highly recommend this book to
people beginning in NLP with Python.
Downloading and installing NLTK
Install NLTK: run pip install nltk
Test installation: run python then type import nltk
For platform-specibc instructions, read here.
Installing NLTK Packages
import NLTK and run nltk.download(). This will open the NLTK downloader
from where you can choose the corpora and models to download. You can
also download all packages at once.
Text Pre- Processing with NLTK
The main issue with text data is that it is all in text format (strings). However,
the Machine learning algorithms need some sort of numerical feature vector
in order to perform the task. So before we start with any NLP project we need
to pre-process it to make it ideal for working. Basic text pre-processing
includes:
Converting the entire text into uppercase or lowercase, so that the
algorithm does not treat the same words in diierent cases as diierent
Tokenization: Tokenization is just the term used to describe the process of
converting the normal text strings into a list of tokens i.e words that we
actually want. Sentence tokenizer can be used to bnd the list of sentences
and Word tokenizer can be used to bnd the list of words in strings.
The NLTK data package includes a pre-trained Punkt tokenizer for English.
Removing Noise i.e everything that isn’t in a standard number or letter.
Removing Stop words. Sometimes, some extremely common words which
would appear to be of little value in helping select documents matching a
user need are excluded from the vocabulary entirely. These words are
called stop words
Stemming: Stemming is the process of reducing incected (or sometimes
derived) words to their stem, base or root form—generally a written word
form. Example if we were to stem the following words: “Stems”,
“Stemming”, “Stemmed”, “and Stemtization”, the result would be a single
word “stem”.
Lemmatization: A slight variant of stemming is lemmatization. The major
diierence between these is, that, stemming can often create non-existent
words, whereas lemmas are actual words. So, your root stem, meaning the
word you end up with, is not something you can just look up in a
dictionary, but you can look up a lemma. Examples of Lemmatization are
that “run” is a base form for words like “running” or “ran” or that the word
“better” and “good” are in the same lemma so they are considered the
same.
Bag of Words
After the initial preprocessing phase, we need to transform text into a
meaningful vector (or array) of numbers. The bag-of-words is a
representation of text that describes the occurrence of words within a
document. It involves two things:
•A vocabulary of known words.
•A measure of the presence of known words.
Why is it is called a “bag” of words? That is because any information about the
order or structure of words in the document is discarded and the model is
only concerned with whether the known words occur in the document,
not where they occur in the document.
The intuition behind the Bag of Words is that documents are similar if they
have similar content. Also, we can learn something about the meaning of the
document from its content alone.
For example, if our dictionary contains the words {Learning, is, the, not,
great}, and we want to vectorize the text “Learning is great”, we would have
the following vector: (1, 1, 0, 0, 1).
TF-IDF Approach
A problem with the Bag of Words approach is that highly frequent words start
to dominate in the document (e.g. larger score), but may not contain as much
“informational content”. Also, it will give more weight to longer documents
than shorter documents.
One approach is to rescale the frequency of words by how often they appear in
all documents so that the scores for frequent words like “the” that are also
frequent across all documents are penalized. This approach to scoring is
called Term Frequency-Inverse Document Frequency, or TF-IDF for short,
where:
Term Frequency: is a scoring of the frequency of the word in the current
document